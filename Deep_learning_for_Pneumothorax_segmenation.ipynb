{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katek28/Deep-Learning-projects/blob/main/Deep_learning_for_Pneumothorax_segmenation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3QdJQhoo71N"
      },
      "source": [
        "# AMV Group Assignment: Deep learning for Pneumothorax segmentation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In this assignment, we experimented with deep learning models for image segmentation. We developed a model to segment a pneumothorax on chest x-ray images, using state-of-the-art models and a public xray dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sVD7r0Lpsnn"
      },
      "source": [
        "# Getting started and setting things up\n",
        "## Importing and installing modules\n",
        "In this notebook we used several custom python modules, that had to be installed before we could use them. This is done in the code cell below. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxb3EbUYowcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4fc7cede-7277-49b0-c417-9803281482db"
      },
      "source": [
        "#@title\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import cv2\n",
        "import copy\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "import shutil\n",
        "import tqdm\n",
        "import zipfile\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from ipywidgets import interact\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "!pip install segmentation-models-pytorch\n",
        "!pip install pytorch-lightning\n",
        "!pip install albumentations\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor\n",
        "lr_monitor = LearningRateMonitor()\n",
        "\n",
        "from pytorch_lightning.metrics.classification import AUROC, Accuracy, F1\n",
        "\n",
        "from albumentations import (\n",
        "    Compose, HorizontalFlip, RandomBrightness, RandomContrast, RandomGamma, OneOf,\n",
        "    ToFloat, ShiftScaleRotate, RandomBrightness, RandomContrast, RandomSizedCrop\n",
        ")\n",
        "\n",
        "def dice_coefficient(y_true, y_pred, empty_score=1.0, mode='hard'):\n",
        "    # Function to calculate dice coefficient after thresholding\n",
        "    if mode == 'hard':\n",
        "        y_th = y_pred > 0.5\n",
        "    elif mode == 'soft':\n",
        "        y_th = y_pred\n",
        "    else:\n",
        "        raise ValueError('Invalid dice mode! Choose either \"soft\" or \"hard\"')\n",
        "    im1 = y_true\n",
        "    im2 = y_th\n",
        "    if im1.shape != im2.shape:\n",
        "        raise ValueError(\"Shape mismatch: im1 and im2 must have the same shape.\")\n",
        "    im_sum = im1.sum() + im2.sum()\n",
        "    if im_sum == 0:\n",
        "        return empty_score\n",
        "    intersection = (im1*im2).sum()\n",
        "\n",
        "    return (2. * intersection.sum()) / im_sum\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "print('Module installations and imports completed successfully!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.2.0-py3-none-any.whl (87 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▊                            | 10 kB 17.5 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 20 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 30 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 40 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 61 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 81 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 87 kB 3.7 MB/s \n",
            "\u001b[?25hCollecting pretrainedmodels==0.7.4\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.4 MB/s \n",
            "\u001b[?25hCollecting efficientnet-pytorch==0.6.3\n",
            "  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (0.10.0+cu111)\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 32.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (1.9.0+cu111)\n",
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.15.0)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12421 sha256=951bafbb4ab880308909bd143c5fee14f287e4ece5d9aa42051e09e0faf2cb9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/6b/0c/f0ad36d00310e65390b0d4c9218ae6250ac579c92540c9097a\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60965 sha256=cd64bd6a84860e522f0c288459255196c4377901e30a8c4eae0df27aaa976650\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, timm, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.6.3 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.2.0 timm-0.4.12\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.5.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.62.3)\n",
            "Collecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Collecting PyYAML>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 33.4 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.11.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 53.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.0)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.9.0+cu111)\n",
            "Collecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n",
            "\u001b[K     |████████████████████████████████| 329 kB 55.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (3.7.4.3)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.6.0)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 40.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 39.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (2.4.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (57.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.41.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.1)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.0-py3-none-any.whl (6.1 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 41.6 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 48.4 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 46.7 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.7)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.6.0)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=d273ba1befec91d26c5525ca165bbc50326510fbb0aad33f02c27d4e5c2c21a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, torchmetrics, PyYAML, pyDeprecate, future, pytorch-lightning\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.8.0 aiosignal-1.2.0 async-timeout-4.0.0 asynctest-0.13.0 frozenlist-1.2.0 fsspec-2021.11.0 future-0.18.2 multidict-5.2.0 pyDeprecate-0.3.1 pytorch-lightning-1.5.0 torchmetrics-0.6.0 yarl-1.7.2\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n",
            "Collecting imgaug<0.2.7,>=0.2.5\n",
            "  Downloading imgaug-0.2.6.tar.gz (631 kB)\n",
            "\u001b[K     |████████████████████████████████| 631 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from albumentations) (4.1.2.30)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.19.5)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (0.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (1.15.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (3.2.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (7.1.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.7)\n",
            "Building wheels for collected packages: imgaug\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.2.6-py3-none-any.whl size=654020 sha256=b6696f50f91b27815db5972ca3d2ab2d89d4f010a10974124a8a507995da13d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/72/98/3ebfdba1069a9a8eaaa7ae7265cfd67d63ef0197aaee2e5f9c\n",
            "Successfully built imgaug\n",
            "Installing collected packages: imgaug\n",
            "  Attempting uninstall: imgaug\n",
            "    Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "Successfully installed imgaug-0.2.6\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9e4f448c73f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mlr_monitor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLearningRateMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAUROC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m from albumentations import (\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning.metrics'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq0utW16rjE1"
      },
      "source": [
        "print(pl.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY9m0Ma1qAr6"
      },
      "source": [
        "### Mounting google drive to the notebook\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P3NkMdjpqHN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "faf9be51-0981-49c3-a52f-e2df222720c5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e5b0420cd036>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m   )\n\u001b[0;32m--> 177\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    102\u001b[0m     ):\n\u001b[1;32m    103\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aypQ1jiZv7u4"
      },
      "source": [
        "### Copy data from drive to colab\n",
        "In the code cell below we import the data into google colab, and extract the .zip file. By copying it into colab instead of loading it from the google drive each time, we can do much faster training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2q9rkE51cWv"
      },
      "source": [
        "#Copying the data to the local colab runtime\n",
        "\n",
        "######################\n",
        "zip_path = '/gdrive/My Drive/dataset.zip'\n",
        "######################\n",
        "\n",
        "print('Copying data to local runtime')\n",
        "shutil.copyfile(zip_path, 'dataset.zip')\n",
        "print('Copy complete. Extracting...')\n",
        "if not os.path.exists('dataset'):\n",
        "    os.makedirs('dataset')\n",
        "!unzip -q dataset.zip -d dataset\n",
        "print('Extraction complete.')\n",
        "print('Dataset imported to CoLab runtime succesfully.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh6whWuYzX17"
      },
      "source": [
        "## Definitions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-y2jQRKtXVZ"
      },
      "source": [
        "### Defining the dataset\n",
        "\n",
        "Here, the 'DataLoader' class is defined. It assumes that the dataset was succesfully copied to the local colab runtime, so make sure you've executed the previous codecell without error.\n",
        "\n",
        "Apart from loading the images, it also handles splitting of the dataset into a train / validation / test parts. Run the code cell below to define our dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uLb-XBCz7XK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "46b5e58e-e6a2-4fbf-83a0-4a2688c01d30"
      },
      "source": [
        "#@title\n",
        "class SIIMACR_kaggle(Dataset):\n",
        "    '''\n",
        "    Dataset Class for SIIM-ACR pneumothorax segmentation dataset from kaggle\n",
        "    Dataset link - https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation/data\n",
        "    There is 1 class in the given labels.\n",
        "    The `get_filenames` function retrieves the filenames of all images in the given `path` and\n",
        "    saves the absolute path in a list.\n",
        "    In the `get_item` function, images and masks are resized to the given `img_size`, \n",
        "    given `transform` (if any) are applied to the image only\n",
        "    (mask does not usually require transforms, but they can be implemented in a similar way).\n",
        "    '''\n",
        "\n",
        "    def __init__(self, root_path, split, img_size=(512, 512), \n",
        "                 transform=None, mode='segmentation', negative_fraction=1):\n",
        "        self.img_size = img_size\n",
        "        self.transform = transform\n",
        "        self.split = split\n",
        "        self.root = root_path\n",
        "        self.df = pd.read_csv(os.path.join(root_path, 'dataset.csv'), index_col=0)\n",
        "        self.mode = mode\n",
        "        self.negative_fraction = negative_fraction\n",
        "\n",
        "        trainingset = self.df[self.df['dataset']=='train']\n",
        "        test = self.df[self.df['dataset']=='test']\n",
        "        # Split between train, valid and test set\n",
        "        train, validation = train_test_split(trainingset, test_size=0.1, random_state=42, stratify=trainingset['pneu'])\n",
        "\n",
        "        datasets = {'train': train, 'valid': validation, 'test': test}\n",
        "        self.samples = self.subsample_negative_samples(datasets[split])\n",
        "        self.data_summary = self.create_data_summary()\n",
        "        self.tensortransform = transforms.Compose([transforms.ToTensor()])\n",
        "   \n",
        "    def __len__(self):\n",
        "        return(len(self.samples))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(os.path.join(self.root, self.samples.iloc[idx]['image_path'].replace('\\\\', '/')))\n",
        "        img = img.resize(self.img_size)\n",
        "        img = np.array(img)\n",
        "\n",
        "        mask = Image.open(os.path.join(self.root, self.samples.iloc[idx]['mask_path'].replace('\\\\', '/')))\n",
        "        mask = mask.resize(self.img_size)\n",
        "        mask = np.array(mask)\n",
        "        mask[mask!=0] = 1\n",
        "        mask = mask[:,:, 0].squeeze()\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=img, mask=mask)\n",
        "        if self.mode == 'segmentation':\n",
        "            if self.transform:\n",
        "                return self.tensortransform(augmented['image']), augmented['mask']\n",
        "            else:\n",
        "                return img, mask\n",
        "        elif self.mode == 'classification':\n",
        "             return img, int(mask.any())\n",
        "        elif self.mode == 'classification+segmentation':\n",
        "             return img, mask, int(mask.any())\n",
        "\n",
        "    def give_data_for_class(self, cclass, idx):\n",
        "        if cclass == 'pneu':\n",
        "            sample_idxs = np.nonzero(self.samples['pneu'].values)[0]\n",
        "            return self.give_data(sample_idxs[idx])\n",
        "        elif cclass == 'no_pneu':\n",
        "            sample_idxs = np.nonzero(self.samples['pneu'].values != 1)[0]\n",
        "            return self.give_data(sample_idxs[idx])\n",
        "\n",
        "    def give_data(self, idx):\n",
        "        img = Image.open(os.path.join(self.root, self.samples.iloc[idx]['image_path'].replace('\\\\', '/')))\n",
        "        img = img.resize(self.img_size)\n",
        "        img = np.array(img)\n",
        "\n",
        "        mask = Image.open(os.path.join(self.root, self.samples.iloc[idx]['mask_path'].replace('\\\\', '/')))\n",
        "        mask = mask.resize(self.img_size)\n",
        "        mask = np.array(mask)\n",
        "        mask[mask!=0] = 1\n",
        "        mask = mask[:,:, 0].squeeze()\n",
        "        return img, mask\n",
        "\n",
        "    def create_data_summary(self):\n",
        "        return {'pneu':len(self.samples[self.samples['pneu']==1]), \n",
        "                'no_pneu':len(self.samples[self.samples['pneu']==0])}\n",
        "    \n",
        "    def subsample_negative_samples(self, dataset):\n",
        "        neg_samples = dataset[dataset['pneu']==0]\n",
        "        pos_samples = dataset[dataset['pneu']==1]\n",
        "        n_neg = int(np.clip(self.negative_fraction*len(neg_samples), 1, None))\n",
        "        neg_samples_sampled = neg_samples.sample(n_neg, random_state=42)\n",
        "        return pd.concat([pos_samples, neg_samples_sampled])\n",
        "\n",
        "print('DataSet defined.')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f63b38127959>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSIIMACR_kaggle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     '''\n\u001b[1;32m      4\u001b[0m     \u001b[0mDataset\u001b[0m \u001b[0mClass\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mSIIM\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mACR\u001b[0m \u001b[0mpneumothorax\u001b[0m \u001b[0msegmentation\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkaggle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mDataset\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mwww\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaggle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msiim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0macr\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpneumothorax\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msegmentation\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5UOW6qsmWGq"
      },
      "source": [
        "#### Inspecting the data\n",
        "A very important aspect of deep learning is making sure that your data is in a proper state before you pass it into your model. If something's wrong with the data, you will never be able to train a good model on it. Therefore, it is really important to visually inspect your data before you do anything with it.\n",
        "\n",
        "Using the cell below, we could have a look at the data and the masks for the training, validation and test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as8WNCBRmxha",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "a4ec800b-e06d-46c6-fd2d-a1bf54c7c202"
      },
      "source": [
        "# You can change the value of 'dataset' to inspect the different splits\n",
        "# Options are: 'train', 'valid' or 'test'\n",
        "dataset = 'train'\n",
        "\n",
        "dsc = SIIMACR_kaggle(root_path='dataset', split=dataset, transform=None, negative_fraction=1)\n",
        "\n",
        "n_total = dsc.__len__()\n",
        "n_pneu = dsc.data_summary['pneu']\n",
        "n_nopneu = dsc.data_summary['no_pneu']\n",
        "\n",
        "print('{} dataset contains {} ({:.1f}%) \"pneu\", and {} ({:.1f}%) \"no_pneu\" samples'.format(dataset, n_pneu, n_pneu/n_total*100, n_nopneu, n_nopneu/n_total*100))\n",
        "\n",
        "@interact(show_class=['pneu', 'no_pneu'], index=(0, dsc.data_summary['no_pneu']-1, 1))\n",
        "def plot_image_and_mask(show_class='pneu', index=0):\n",
        "    image, mask = dsc.give_data_for_class(show_class, index)\n",
        "    image_masked = copy.deepcopy(image)\n",
        "    image_masked[:,:,0][mask!=0] = image_masked[:,:,0][mask!=0]*1\n",
        "    image_masked[:,:,1][mask!=0] = image_masked[:,:,1][mask!=0]*0\n",
        "    image_masked[:,:,2][mask!=0] = image_masked[:,:,2][mask!=0]*0\n",
        "\n",
        "    display_image = np.hstack((image, image_masked))\n",
        "    fig = plt.figure(figsize=(8,4))\n",
        "    plt.imshow(display_image, cmap='bone')\n",
        "    plt.axis('off')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-fa1416e3704a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSIIMACR_kaggle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_fraction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mn_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SIIMACR_kaggle' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3K9QRtSfe_g"
      },
      "source": [
        "### Defining the loss functions and metrics\n",
        "\n",
        "For this segmentation problem we used the dice coefficient as a metric. It measures the overlap between two areas; in our case these are the labels and the predictions of the model. For perfect overlap, the dice coefficient is equal to 1. If there is no overlap, the dice is 0.\n",
        "\n",
        "We experimented with three different loss functions: The dice coefficient, the binary cross entropy and the combo-loss, which is just defined as the sum of the bce loss and the dice loss\n",
        "\n",
        "The Dice loss and the Combo loss are defined below. The bce is one of the default loss functions available in pytorch, so we don't have to define it manually here. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfCd-R2Rffee"
      },
      "source": [
        "#@title\n",
        "def DiceMetric(inputs, targets, smooth=1): \n",
        "    #flatten label and prediction tensors\n",
        "    inputs = inputs.view(-1)\n",
        "    targets = targets.view(-1)\n",
        "\n",
        "    intersection = (inputs * targets).sum()                            \n",
        "    dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
        "\n",
        "    return dice\n",
        "\n",
        "def DiceLoss(inputs, targets, smooth=1):\n",
        "    return 1 - DiceMetric(inputs, targets, smooth)\n",
        "\n",
        "def ComboLoss(inputs, targets, smooth=1):\n",
        "    dice_contribution = DiceLoss(inputs, targets, smooth)\n",
        "    bce_contribution = F.binary_cross_entropy(inputs, targets)\n",
        "    return bce_contribution + dice_contribution\n",
        "\n",
        "print('Loss functions defined.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv6UnYUjyuZb"
      },
      "source": [
        "### Defining the model\n",
        "\n",
        "In the following code cell the model itself is defined. Model definition is still a bit complex because it requires not only the details of the model itself, but also procedures for calculating the loss and metrics during training, validation and testing. Furthermore, it also holds the configuration of the optimizers and hyperparameters used during training (e.g. learning rate, number of epochs, batch size, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zHetDOlsfEz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "8eba7b3f-0be4-4ec3-82b5-3cb2e48d9997"
      },
      "source": [
        "#@title\n",
        "class Model(pl.LightningModule):\n",
        "    '''\n",
        "    Model Module\n",
        "    This is a basic  module implemented with Pytorch Lightning.\n",
        "    It is specific to the SIIMACR dataset i.e. dataloaders are for this thorax xray dataset\n",
        "    It uses the ResNet18 model as an example.\n",
        "    Adam optimizer is used.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "        self.hparams.update(hparams)\n",
        "        self.root_path = hparams['root']\n",
        "        self.batch_size = 4\n",
        "        self.epochs = hparams['epochs']\n",
        "        self.learning_rate = hparams['lr']\n",
        "        self.scheduler = hparams['lr_scheduler']\n",
        "        self.loss_function = hparams['loss_function']\n",
        "        self.mode='segmentation'\n",
        "        self.negative_fraction=hparams['negative_fraction']\n",
        "        self.augmentation = hparams['data_augmentation']\n",
        "\n",
        "        decoder_channels = [256, 128, 64, 32, 16]\n",
        "        self.net = smp.Unet(hparams['model_backbone'],\n",
        "                              encoder_depth=hparams['encoder_depth'], \n",
        "                              encoder_weights = None,\n",
        "                              classes=1, \n",
        "                              in_channels=3, \n",
        "                              activation='sigmoid', \n",
        "                              aux_params = None,\n",
        "                              decoder_channels=decoder_channels[:hparams['encoder_depth']]) \n",
        "\n",
        "        self.transform_test = Compose([ToFloat(max_value=1)],p=1)\n",
        "\n",
        "        if self.augmentation:\n",
        "            self.transform_train = Compose([\n",
        "                  HorizontalFlip(p=0.5),\n",
        "                  ShiftScaleRotate(shift_limit=0.0825, scale_limit=0.2, rotate_limit=25),\n",
        "                  OneOf([\n",
        "                    RandomContrast(),\n",
        "                    RandomGamma(),\n",
        "                    RandomBrightness(),\n",
        "                    ], p=0.3),\n",
        "                  ToFloat(max_value=1)\n",
        "                ],p=1)\n",
        "        else:\n",
        "            self.transform_train = self.transform_test\n",
        "       \n",
        "        self.trainset = SIIMACR_kaggle(self.root_path, split='train', transform=self.transform_train, mode=self.mode, negative_fraction=self.negative_fraction)\n",
        "        self.validset = SIIMACR_kaggle(self.root_path, split='valid', transform=self.transform_test, mode=self.mode, negative_fraction=1)\n",
        "        self.testset = SIIMACR_kaggle(self.root_path, split='test', transform=self.transform_test, mode=self.mode, negative_fraction=1)\n",
        "\n",
        "        self.ntest = self.testset.__len__()\n",
        "        self.nvalid = self.validset.__len__()\n",
        "        self.ntrain = self.trainset.__len__()\n",
        "        \n",
        "        self.metric = 'dice'\n",
        "        \n",
        "        self.save_hyperparameters()\n",
        "    \n",
        "    def on_fit_start(self):\n",
        "        metric_placeholder = {'test_{}'.format(self.metric): 0, 'val_{}'.format(self.metric): 0}\n",
        "        self.logger.log_hyperparams(self.hparams, metrics=metric_placeholder)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def calculate_loss_and_metric(self, batch):\n",
        "        img, mask = batch\n",
        "        img = img.float()\n",
        "        mask = mask.float().unsqueeze(1)\n",
        "        out = self(img)\n",
        "        if self.loss_function == 'dice':\n",
        "            loss_val = DiceLoss(out, mask)\n",
        "        elif self.loss_function == 'bce':\n",
        "            loss_val = F.binary_cross_entropy(out, mask)\n",
        "        elif self.loss_function == 'combo':\n",
        "            loss_val = ComboLoss(out, mask)\n",
        "        dice = DiceMetric(out, mask)\n",
        "        return loss_val, dice\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        loss_val, metric = self.calculate_loss_and_metric(batch)\n",
        "        log_dict = {'train_loss': loss_val, 'train_{}'.format(self.metric): metric}\n",
        "        self.log_dict(log_dict)\n",
        "        return {'loss': loss_val, 'log': log_dict, 'progress_bar': log_dict}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss_val, metric = self.calculate_loss_and_metric(batch)\n",
        "        log_dict = {'val_loss': loss_val, 'val_{}'.format(self.metric): metric}\n",
        "        self.log_dict(log_dict)\n",
        "        return {'val_loss': loss_val, 'val_{}'.format(self.metric): metric}\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss_val, metric = self.calculate_loss_and_metric(batch)\n",
        "        log_dict = {'test_loss': loss_val, 'test_{}'.format(self.metric): metric}\n",
        "        self.log_dict(log_dict)\n",
        "        return {'test_{}'.format(self.metric): metric}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        loss_val = sum(output['val_loss'] for output in outputs) / len(outputs)\n",
        "        metric_val = sum(output['val_{}'.format(self.metric)] for output in outputs) / len(outputs)\n",
        "        log_dict = {'val_loss': loss_val, 'val_{}'.format(self.metric): metric_val}\n",
        "        self.log_dict(log_dict)\n",
        "        return {'log': log_dict, 'val_loss': log_dict['val_loss'], 'progress_bar': log_dict, 'val_{}'.format(self.metric): log_dict['val_{}'.format(self.metric)]}\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        metric_val = sum(output['test_{}'.format(self.metric)] for output in outputs) / len(outputs)\n",
        "        log_dict = {'test_{}'.format(self.metric): metric_val}\n",
        "        self.log_dict(log_dict)\n",
        "        return {'log': log_dict, 'progress_bar': log_dict, 'test_{}'.format(self.metric): log_dict['test_{}'.format(self.metric)]}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        opt = torch.optim.Adam(self.net.parameters(), lr=self.learning_rate)\n",
        "        if self.scheduler == 'cosine':\n",
        "            sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=self.epochs/3)\n",
        "        else:\n",
        "            lmbd = lambda epoch: 1\n",
        "            sch = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lmbd)\n",
        "        return [opt], [sch]\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.trainset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.validset, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.testset, batch_size=self.batch_size, shuffle=False, num_workers=1)\n",
        "\n",
        "print('Model class defined.')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8b7b50db1e64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     '''\n\u001b[1;32m      4\u001b[0m     \u001b[0mModel\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mbasic\u001b[0m  \u001b[0mmodule\u001b[0m \u001b[0mimplemented\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mPytorch\u001b[0m \u001b[0mLightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pl' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MKD6AHr4-jC"
      },
      "source": [
        "# Model Training\n",
        "\n",
        "Now that everything's set up, we get to the fun part: actually training and evaluating the model. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3dst9nJtCEy"
      },
      "source": [
        "## Specify hyperparameters\n",
        "Here, you can set the hyperparameters and the model name. \n",
        "\n",
        "There are several new hyperparameters here that were not in the week 3 code assignment. The meaning of those parameters is explained in more detail at the bottom of this notebook (in the 'Assignment' section)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sljvr7-s4_B5"
      },
      "source": [
        "# Here, the hyperparameters for the model are defined. \n",
        "\n",
        "hparams = {\n",
        "            'root': 'dataset',\n",
        "            'epochs': 10,\n",
        "            'encoder_depth': 5,\n",
        "            'lr': 1e-2,\n",
        "            'lr_scheduler': 'constant',\n",
        "            'loss_function': 'dice',\n",
        "            'model_backbone': 'resnet18',\n",
        "            'negative_fraction': 0.0,\n",
        "            'data_augmentation': False\n",
        "           }\n",
        "\n",
        "# The name of the model that will be trained. Make sure to change this before \n",
        "# starting a next training run, to avoid overwriting your previously trained \n",
        "# models!\n",
        "model_name = 'resnet18_version_0'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZbsRSrT44BS"
      },
      "source": [
        "## The training procedure\n",
        "\n",
        "The cell below contains the code to run the model training. Once you run it, it will create folders to store the model weights. It will also show you how the training is progressing.\n",
        "\n",
        "After each epoch, the model is evaluated on the validation set. If the dice coefficient on the validation set improved, a model 'checkpoint' is created, which just means that the current weights of the model are saved to disk. \n",
        "\n",
        "After the training completes and the model has been trained for the specified number of epochs, the latest model checkpoint is loaded (so the 'best possible' model that was trained is used) and that model is used to make predictions on the test set. The dice coefficient on the test set is then calculated.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfi_rG6G44oH"
      },
      "source": [
        "# 1 INIT LIGHTNING MODEL\n",
        "\n",
        "model = Model(hparams)\n",
        "\n",
        "# 2 Create folder to save the models\n",
        "checkpoint_path = os.path.join(os.getcwd(), 'pytorch_checkpoints', model_name)\n",
        "if not os.path.exists(checkpoint_path):\n",
        "    os.makedirs(checkpoint_path)\n",
        "\n",
        "checkpoint=ModelCheckpoint(dirpath=checkpoint_path, filename=model_name+'_'+'{epoch}', \n",
        "                                        save_top_k=1, verbose=True, monitor='val_dice', mode='max')\n",
        "\n",
        "# 3 INIT TRAINER\n",
        "trainer = pl.Trainer(\n",
        "    gpus=1,\n",
        "    max_epochs=hparams['epochs'],\n",
        "    checkpoint_callback=True,\n",
        "    callbacks=[checkpoint,lr_monitor],\n",
        "    )\n",
        "\n",
        "# 4 START TRAINING\n",
        "trainer.fit(model)\n",
        "\n",
        "# 5 Evaluate model on test set\n",
        "trainer.test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt5HqwIeuLcc"
      },
      "source": [
        "## Monitoring the training\n",
        "\n",
        "Execute the cell below to fire up Tensorboard (wait for a minute for the app to load). \n",
        "\n",
        "Remember to set 'Smoothing' to zero, and that you don't have to rerun the cell to load new training results, you can just press the 'Refresh' button in Tensorboard.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxwVyfp8OiUN"
      },
      "source": [
        "#@title\n",
        "%tensorboard --logdir lightning_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yozu8Raytrkl"
      },
      "source": [
        "# Model Evaluation\n",
        "\n",
        "To evaluate model performance, performance metrics are used. For a segmentation task such as here, a popular metric is the dice coefficient. Metrics are a very convenient way to measure performance because they can easily be averaged over the entire dataset (or the train / validation / test sets). They therefore allow you to summarize model performance with a single scalar. \n",
        "\n",
        "However, in order to get a feeling for model performance it is equally important to visually inspect the predictions from the model, to see if they make sense. In this section, we both visualized the predictions and calculated the mean dice coefficients for the different subsets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7gcygxy87cN"
      },
      "source": [
        "## Visualizing the results\n",
        "\n",
        "Execute the code cell below. This will bring up a widget in which you can select a model checkpoint and a dataset to evaluate. Use the dropdown boxes to select an appropriate checkpoint and dataset (for instance the test or validation set). \n",
        "\n",
        "Next, you can use the slider to walk through the images in the dataset and inspect the image (left), mask (middle) and model prediction (right) for each image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKPWft7P8UjO"
      },
      "source": [
        "#@title\n",
        "# -----------------------------------\n",
        "# HELPER FUNCTIONS FOR VISUALIZATION\n",
        "# -----------------------------------\n",
        "\n",
        "def predict_on_image(pytorch_model, index=0, dataset='test'):\n",
        "  # Function to generate a prediction using pytorch_model,\n",
        "  # on any of the images from the test set\n",
        "    dataset_map = {'test':pytorch_model.testset, \n",
        "                 'validation': pytorch_model.validset, \n",
        "                 'train': pytorch_model.trainset}\n",
        "    ds = dataset_map[dataset]\n",
        "    img, mask = ds.__getitem__(index)\n",
        "    pred = pytorch_model.eval()(img.float().cuda(device=0).unsqueeze(0))\n",
        "    pred = pred.cpu().detach().numpy().squeeze()\n",
        "    return img.cpu().squeeze().numpy()/255, mask, pred\n",
        "\n",
        "\n",
        "def predict_and_plot(pytorch_model, index=0, dataset='test', dice_mode='soft'):\n",
        "    img, mask, pred = predict_on_image(pytorch_model, index, dataset)\n",
        "    dice = dice_coefficient(mask, pred, mode=dice_mode)\n",
        "    print('Dice coefficient for {} image {} is: {:.3f}'.format(dataset, index, dice))\n",
        "    image = np.moveaxis(img, 0, -1)\n",
        "    image_masked = copy.deepcopy(image)\n",
        "    image_predicted = copy.deepcopy(image)\n",
        "    prediction = pred > 0.5\n",
        "\n",
        "    image_masked[:,:,0][mask!=0] = image_masked[:,:,0][mask!=0]*1\n",
        "    image_masked[:,:,1][mask!=0] = image_masked[:,:,1][mask!=0]*0\n",
        "    image_masked[:,:,2][mask!=0] = image_masked[:,:,2][mask!=0]*0\n",
        "\n",
        "    image_predicted[:,:,0][prediction!=0] = image_predicted[:,:,0][prediction!=0]*0\n",
        "    image_predicted[:,:,1][prediction!=0] = image_predicted[:,:,1][prediction!=0]*0\n",
        "    image_predicted[:,:,2][prediction!=0] = image_predicted[:,:,2][prediction!=0]*1\n",
        "\n",
        "    display_image = np.hstack((image, image_masked, image_predicted))\n",
        "    fig = plt.figure(figsize=(24,8))\n",
        "    plt.imshow(display_image, cmap='bone')\n",
        "    plt.axis('off')\n",
        "    return dice, fig \n",
        "\n",
        "\n",
        "#cps = os.listdir(os.path.join(os.getcwd(), 'pytorch_checkpoints'))\n",
        "cps = glob(os.path.join(checkpoint_path, '*.ckpt'))\n",
        "datasets = ['train', 'validation', 'test']\n",
        "\n",
        "@interact(checkpoint=cps, dataset=datasets)\n",
        "def select_model_checkpoint(checkpoint=cps[0], dataset='validation'):\n",
        "    global eval_model, ds, model_checkpoint\n",
        "    model_checkpoint = checkpoint\n",
        "    ckpth_path = checkpoint\n",
        "    eval_model = Model.load_from_checkpoint(ckpth_path)\n",
        "    eval_model.cuda(device=0)\n",
        "    ds = dataset\n",
        "\n",
        "dsmap = {'test':eval_model.ntest, 'validation':eval_model.nvalid, 'train':eval_model.ntrain}\n",
        "@interact\n",
        "def plot_sample(index=(0,dsmap[ds]-1,1)):\n",
        "    print('Evaluating model from checkpoint: {}'.format(model_checkpoint))\n",
        "    predict_and_plot(pytorch_model=eval_model, index=index, dataset=ds, dice_mode='hard')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr2--0rQtxUZ"
      },
      "source": [
        "## The mean dice coefficient\n",
        "The code below calculates the dice coefficient for the model (loaded from the checkpoint we selected with the widget above) for the training, validation and test datasets. To do so, predictions were generated for all images in the datasets, so the code takes some time to execute. \n",
        "\n",
        "In addition to the means, a histogram is generated that shows the distribution of dice scores for the train, validation and test set. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVewLSHdbcN6"
      },
      "source": [
        "#@title\n",
        "print('Evaluating model from checkpoint: {}'.format(model_checkpoint))\n",
        "evaldices = pd.DataFrame({})\n",
        "for dsname, dslen in dsmap.items():\n",
        "    for index in range(dslen):\n",
        "        _, mask, pred = predict_on_image(eval_model, index, dataset=dsname)\n",
        "        evaldices.at[index, 'dice_{}'.format(dsname)] = dice_coefficient(mask, pred, mode='hard') \n",
        "\n",
        "    print('Mean dice coefficient on the {} set is {:.3f}'.format(dsname, evaldices['dice_{}'.format(dsname)].mean()))\n",
        "\n",
        "fig = go.Figure()\n",
        "for dsname, _ in dsmap.items():\n",
        "    fig.add_trace(go.Histogram(x=evaldices['dice_{}'.format(dsname)], name=dsname, xbins={'start':0.0, 'end': 1.05, 'size':0.05}))\n",
        "\n",
        "    fig.update_layout(\n",
        "    title_text='Histogram of hard dice scores across datasets, calculated from model checkpoint: {}'.format(model_checkpoint), \n",
        "    xaxis_title_text='Dice score', \n",
        "    yaxis_title_text='Count', \n",
        "    bargap=0.1, \n",
        "    bargroupgap=0.025,\n",
        "    hovermode=\"x\"\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OwdZ2kjLzG1"
      },
      "source": [
        "At this point we noticed two things:\n",
        "\n",
        "\n",
        "1.   The mean dice scores calculated here are different from what is shown during the training and in Tensorboard\n",
        "2.   There are peaks in the distribution of dice coefficients at a dice score of 0 (large peak) and 1 (small peak).\n",
        "\n",
        "\n",
        "Observation #1 has to do with the way the dice coefficient is calculated. During training, dice is calculated using the model predictions (i.e. the probability values per pixel) directly. That means that if a certain pixel gets a probability of for example 0.67, it also counts for only 0.67 of overlap with the ground truth (provided it is a pixel that is positive in the ground truth). \n",
        "\n",
        "\n",
        "However, this is not the way the dice coefficient is formally defined: It should be calculated on *binary* predictions, i.e. we should first apply a threshold to the predicted probabilities for all pixels. I have applied a threshold of 0.5 to calculate the means and histograms above. \n",
        "\n",
        "To differentiate between the two methods, the method when dice is calculated from the predictions directly is known as the *soft dice coefficient*, whereas when a threshold is first applied is called the *hard dice coefficient*. While the soft dice is useful during training, the hard dice is the accepted measure to report model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlGSdPexNnEJ"
      },
      "source": [
        "## Exporting predictions and masks for the test set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ty4leLegTmQ"
      },
      "source": [
        "# Make sure to set this path to the folder on your google drive in which you want \n",
        "# the exported files to show up. The results will be copied there as a .zip file.\n",
        "\n",
        "google_drive_path = '/gdrive/My Drive/'\n",
        "\n",
        "print('Generating predictions from checkpoint: {}'.format(model_checkpoint))\n",
        "dices = pd.DataFrame({})\n",
        "\n",
        "target_path = os.path.join('export', model_checkpoint)\n",
        "\n",
        "if not os.path.exists(target_path):\n",
        "  os.makedirs(target_path)\n",
        "\n",
        "folders = ['images', 'predictions', 'ground_truth']\n",
        "\n",
        "for folder in folders:\n",
        "    target_folder = os.path.join(target_path, folder)\n",
        "    if not os.path.exists(target_folder):\n",
        "        os.makedirs(target_folder)\n",
        "\n",
        "for index in range(eval_model.ntest):\n",
        "    image, mask, pred = predict_on_image(eval_model, index, dataset='test')\n",
        "    dice = dice_coefficient(mask, pred, mode='soft')\n",
        "    filename = '{:04d}.png'.format(index)\n",
        "    image = np.moveaxis(image, 0, -1)\n",
        "    cv2.imwrite(os.path.join(target_path, 'images', filename), (255*image).astype('uint8'))\n",
        "    cv2.imwrite(os.path.join(target_path, 'predictions', filename), (255*pred).astype('uint8'))\n",
        "    cv2.imwrite(os.path.join(target_path, 'ground_truth', filename), (255*mask).astype('uint8'))\n",
        "    dices.at[filename, 'dice'] = dice\n",
        "    if (index+1) % 300 == 0:\n",
        "        print('Generated predictions for {} images'.format(index+1))\n",
        "    \n",
        "print('Predictions completed. Compressing files....')\n",
        "dices.to_csv(os.path.join(target_path, 'dice_coefficients.csv'))\n",
        "shutil.make_archive('export/{}'.format(model_checkpoint), 'zip', target_path)\n",
        "\n",
        "print('Copying archive to google drive')\n",
        "shutil.copyfile('export/{}.zip'.format(model_checkpoint), os.path.join(google_drive_path, '{}.zip'.format(model_checkpoint)))\n",
        "print('Copy completed.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uWOMRfM2pOe"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "**We needed to improve the model, by experimentally determining better choices for the hyperparameters.** \n",
        "\n",
        "Use the experience and insights you already have from the week 3 coding assignment!\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Hyperparameter interpretation**\n",
        "\n",
        "These are the hyperparameters:\n",
        "\n",
        "*   Number of epochs\n",
        "*   Learning rate\n",
        "*   Negative fraction (value between 0 and 1)\n",
        "*   Loss function (valid choices are 'bce', 'dice' or 'combo')\n",
        "*   Model backbone (valid choices include 'resnet18', 'resnet34', 'efficientnet-b4', and much more)\n",
        "*   Data augmentation (True or False). \n",
        "\n",
        "##### Negative fraction\n",
        "The 'negative fraction' hyperparameter controls the amount of images **without** any pneumothorax in the *training* dataset. A value of 1 means that an equal amount of images with and without pneumothorax are used. A value of 0 means that only images with pneumothorax are used during training. Note that the test and validation set contain both images with and without pneumothorax.\n",
        "\n",
        "##### Model backbone\n",
        "The 'model backbone' hyperparameter controls the structure of the model itself: ResNet18 is a simple, relatively shallow architecture that consists of 18 layers, while ResNet34 is deeper, consisting of 34 layers. Deeper models can lead to better results, but are also more prone to overfitting. For a full list of all available backbones, see https://github.com/qubvel/segmentation_models.pytorch . I recommend that you focus on one or two model families (for example, resnet and efficientnet) and experiment a bit with those. **Note:** If you're using different backbones, make sure to (roughly) explain in your methods sections what the differences are!\n",
        "\n",
        "##### Data augmentation\n",
        "When set to true, certain random transforms will be applied to the data at every epoch with a pre-defined probability. The transformations that are applied are: HorizontalFlip (p=0.5), Shifting, Scaling, Rotation (within certain limits) and a random Brightness, Contrast or Gamma adjustment (also within certain limits). This might help to make the model more generalizable, because it increases diversity in the data. "
      ]
    }
  ]
}